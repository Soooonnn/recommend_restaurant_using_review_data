{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# KoBART 모델 및 토크나이저 로드\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v2')\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2', \n",
    "                                                    bos_token='</s>', eos_token='</s>', \n",
    "                                                    pad_token='<pad>')\n",
    "\n",
    "# 단어 내 반복 글자 및 중복 단어 제거 함수\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):  # NaN 값 처리\n",
    "        return \"\"\n",
    "    text = str(text)  # 문자열로 변환\n",
    "    words = text.split()\n",
    "\n",
    "    # 단어 내 반복 글자 제거\n",
    "    def remove_repeated_patterns(word):\n",
    "        pattern = re.compile(r\"(.{1,1})\\1+\")\n",
    "        return pattern.sub(r\"\\1\", word)\n",
    "\n",
    "    processed_words = [remove_repeated_patterns(word) for word in words]\n",
    "\n",
    "    # 전체 단어에서 중복 제거\n",
    "    unique_words = []\n",
    "    for word in processed_words:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "\n",
    "    return ' '.join(unique_words)\n",
    "\n",
    "# 리뷰 전처리 함수\n",
    "def preprocess_reviews(reviews):\n",
    "    processed_reviews = []\n",
    "    for review in reviews:\n",
    "        review = preprocess_text(review)  # 텍스트 전처리 적용\n",
    "        review = re.sub(r'[^\\w\\sㄱ-ㅎㅏ-ㅣ가-힣]', '', str(review))  # 특수문자 제거\n",
    "        review = re.sub(r'\\s+', ' ', review.strip())  # 공백 정리\n",
    "        processed_reviews.append(review)\n",
    "    return list(set(processed_reviews))  # 중복 제거\n",
    "\n",
    "# 토큰 기준으로 리뷰 그룹화\n",
    "def group_reviews_by_tokens(reviews, tokenizer, max_tokens=512):\n",
    "    grouped_reviews = []\n",
    "    current_group = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    for review in reviews:\n",
    "        token_count = len(tokenizer.encode(review, truncation=False))\n",
    "        \n",
    "        if current_token_count + token_count > max_tokens:\n",
    "            if current_group:\n",
    "                grouped_reviews.append(\" \".join(current_group))\n",
    "            current_group = [review]\n",
    "            current_token_count = token_count\n",
    "        else:\n",
    "            current_group.append(review)\n",
    "            current_token_count += token_count\n",
    "\n",
    "    if current_group:\n",
    "        grouped_reviews.append(\" \".join(current_group))\n",
    "    \n",
    "    return grouped_reviews\n",
    "\n",
    "# 중복 문장 및 단어 제거 함수\n",
    "def clean_text(text):\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    sentence_counts = Counter(sentences)\n",
    "    unique_sentences = [sentence for sentence in sentences if sentence_counts[sentence] == 1]\n",
    "    text = \". \".join(unique_sentences) + \".\"\n",
    "\n",
    "    words = text.split()\n",
    "    filtered_words = []\n",
    "    last_word = None\n",
    "    for word in words:\n",
    "        if word != last_word:\n",
    "            filtered_words.append(word)\n",
    "            last_word = word\n",
    "    text = \" \".join(filtered_words)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 요약 함수\n",
    "def summarize_groups(groups, model, tokenizer, max_input_length=512, max_output_length=200):\n",
    "    summaries = []\n",
    "    for group in groups:\n",
    "        if not group.strip():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            group = preprocess_text(group)\n",
    "            input_ids = tokenizer.encode(group, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n",
    "            summary_ids = model.generate(input_ids, max_length=max_output_length, num_beams=6, early_stopping=True)\n",
    "            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            summary = clean_text(summary)\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing group: {e}\")\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "# 재귀적으로 요약 그룹화 및 요약\n",
    "def recursive_group_and_summarize(groups, model, tokenizer, max_tokens=512, max_input_length=512, max_output_length=200):\n",
    "    while len(groups) > 1:\n",
    "        combined_text = \" \".join(groups)\n",
    "        combined_text = clean_text(combined_text)\n",
    "        new_groups = group_reviews_by_tokens(combined_text.split(\". \"), tokenizer, max_tokens=max_tokens)\n",
    "        groups = summarize_groups(new_groups, model, tokenizer, max_input_length, max_output_length)\n",
    "\n",
    "    final_summary = groups[0] if groups else \"\"\n",
    "    if len(tokenizer.encode(final_summary, truncation=False)) > max_output_length:\n",
    "        final_summary = summarize_groups([final_summary], model, tokenizer, max_input_length, max_output_length)[0]\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "# 엑셀 파일 읽기 및 실행\n",
    "file_path = \"압구정찐최종데이터.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# 모든 가게 요약\n",
    "store_names = data['가게명'].unique()\n",
    "results = []\n",
    "\n",
    "for store_name in store_names:\n",
    "    store_reviews = data[data['가게명'] == store_name]['리뷰']\n",
    "    processed_reviews = preprocess_reviews(store_reviews)\n",
    "    grouped_reviews = group_reviews_by_tokens(processed_reviews, tokenizer, max_tokens=512)\n",
    "    group_summaries = summarize_groups(grouped_reviews, model, tokenizer)\n",
    "    final_summary = recursive_group_and_summarize(group_summaries, model, tokenizer, max_tokens=512)\n",
    "    final_summary = clean_text(final_summary)\n",
    "    results.append({'가게명': store_name, '리뷰 요약': final_summary})\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 엑셀로 저장\n",
    "output_file_path = \"1모든_가게_리뷰_요약.xlsx\"\n",
    "results_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Processed summaries saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "#반복적인 언어가 처리되지 못해 한번더 처리해줬음.\n",
    "def preprocess_text(text):\n",
    "    # 1. 텍스트를 띄어쓰기 기준으로 나눔\n",
    "    words = text.split()\n",
    "\n",
    "    # 2. 단어 내 반복 글자 제거\n",
    "    def remove_repeated_patterns(word):\n",
    "        # 최대 3글자까지 반복되는 패턴 제거\n",
    "        pattern = re.compile(r\"(.{1,2})\\1+\")\n",
    "        return pattern.sub(r\"\\1\", word)\n",
    "\n",
    "    processed_words = [remove_repeated_patterns(word) for word in words]\n",
    "\n",
    "    # 3. 전체 단어에서 중복 제거 (고유 단어만 유지)\n",
    "    unique_words = []\n",
    "    for word in processed_words:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "\n",
    "    # 4. 결과를 다시 텍스트로 변환\n",
    "    return ' '.join(unique_words)\n",
    "\n",
    "# 엑셀 파일 읽기\n",
    "file_path = '압구정_최종_요약.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# 리뷰 요약 전처리 적용\n",
    "data['전처리된 리뷰 요약'] = data['리뷰 요약'].apply(preprocess_text)\n",
    "\n",
    "# 결과를 새로운 엑셀 파일로 저장\n",
    "output_path = '압구정_전처리된_가게_리뷰.xlsx'\n",
    "data.to_excel(output_path, index=False)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"전처리가 완료되었습니다. 결과는 {output_path}에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
